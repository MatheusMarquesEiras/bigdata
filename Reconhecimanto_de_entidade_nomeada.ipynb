{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7qYGKfoMtZDR",
        "outputId": "ec419a2a-f0b7-45ca-ff00-6e3231896a5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"sentence #: {data['Sentence #'].count()}\")\n",
        "print(f\"word: {data['Word'].count()}\")\n",
        "print(f\"pos: {data['POS'].count()}\")\n",
        "print(f\"tag: {data['Tag'].count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_all_pos = data['POS'].unique()\n",
        "data['POS'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_chars_unused = [item for item in list_all_pos.tolist() if not item.isupper()]\n",
        "pos_chars_unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sphVo4y7tqOj"
      },
      "outputs": [],
      "source": [
        "data_fillna = data.ffill(axis=0)  # substitui fillna(method='ffill') metodo usado para preencher para frente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_fillna.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coninuar aqui\n",
        "data_group = (\n",
        "    data_fillna\n",
        "      .groupby('Sentence #', as_index=False)[['Word', 'POS', 'Tag']]\n",
        "      .agg(list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data_group['Sentence #'].count())\n",
        "print(data_group['Word'].count())\n",
        "print(data_group['POS'].count())\n",
        "print(data_group['Tag'].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supondo que a coluna 'Word' contenha listas de strings\n",
        "data_group['Phase'] = data_group['Word'].apply(lambda x: ' '.join(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)  # ou um número grande\n",
        "print(data_group['Phase'].tail(1).to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_dublespaces(text, char):\n",
        "    if f' {char} ' in text:\n",
        "        text = text.replace(f' {char} ', f'{char} ')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, row in data_group['Phase'].items():\n",
        "    for char in pos_chars_unused:\n",
        "        data_group.at[idx, 'Phase'] = remove_dublespaces(row, char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cria a coluna Position como lista vazia\n",
        "data_group['Position'] = [[] for _ in range(len(data_group))]\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    positions = []\n",
        "    for idxW, word in enumerate(row['Word']):\n",
        "        try:\n",
        "            index = row['Phase'].index(word)\n",
        "            lenWord = len(word)\n",
        "            positions.append((index, index + lenWord, row['POS'][idxW]))\n",
        "        except ValueError:\n",
        "            # Palavra não encontrada na Phase\n",
        "            pass\n",
        "    data_group.at[idx, 'Position'] = positions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group['Position'].head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pos_chars_unused\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    items = row['Position']\n",
        "    frase = list(row['Phase'])\n",
        "    for item in items:\n",
        "        if item[2] in pos_chars_unused:\n",
        "            index = item[0]\n",
        "            char = item[2]\n",
        "            frase[index] = char\n",
        "            row['Position'] = frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def achar_posicoes(row):\n",
        "    frase = row['Phase']\n",
        "    palavras = row['Word'] if isinstance(row['Word'], list) else [row['Word']]\n",
        "    pos_tags = row['POS'] if isinstance(row['POS'], list) else [row['POS']]\n",
        "\n",
        "    posicoes = []\n",
        "    start = 0  # garante busca progressiva para lidar com palavras repetidas\n",
        "    for w, pos in zip(palavras, pos_tags):\n",
        "        i = frase.find(w, start)\n",
        "        if i == -1:  # palavra não encontrada\n",
        "            posicoes.append((None, None, pos))\n",
        "        else:\n",
        "            posicoes.append((i, len(w) + i, pos))\n",
        "            start = i + len(w)\n",
        "    return posicoes\n",
        "\n",
        "# Cria/atualiza a coluna de forma vetorizada e segura\n",
        "data_group['Position'] = data_group.apply(achar_posicoes, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group.tail(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group['Phase'].tail(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset = []\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    list_finished_dataset.append((row[\"Phase\"], {\"entities\": row[\"Position\"]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cut = int(len(list_finished_dataset) * 0.1)\n",
        "\n",
        "DEV_DATA = list_finished_dataset[:cut]\n",
        "TRAIN_DATA = list_finished_dataset[cut:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA = [item for item in TRAIN_DATA if '\\x85' not in item[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEV_DATA = [item for item in DEV_DATA if '\\x85' not in item[0] and '\\x94' not in item[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U spacy>=3.7\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1) Dados mock (corrigidos)\n",
        "# -----------------------------\n",
        "TRAIN_DATA = [\n",
        "    (\"João mora em São Paulo.\", {\"entities\": [(0, 4, \"PER\"), (13, 22, \"LOC\")]}),\n",
        "    (\"Maria trabalha na Google em Belo Horizonte.\", {\"entities\": [(0, 5, \"PER\"), (18, 24, \"ORG\"), (28, 42, \"LOC\")]}),\n",
        "    (\"A Apple comprou a Embraer.\", {\"entities\": [(2, 7, \"ORG\"), (18, 25, \"ORG\")]}),\n",
        "    (\"A Ana visitou o Rio de Janeiro.\", {\"entities\": [(2, 5, \"PER\"), (16, 30, \"LOC\")]}),\n",
        "    (\"Pedro nasceu em Lisboa.\", {\"entities\": [(0, 5, \"PER\"), (16, 22, \"LOC\")]}),\n",
        "    (\"A Microsoft abriu escritório em Recife.\", {\"entities\": [(2, 11, \"ORG\"), (32, 38, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "DEV_DATA = [\n",
        "    (\"Carla foi para Porto Alegre ontem.\", {\"entities\": [(0, 5, \"PER\"), (15, 27, \"LOC\")]}),\n",
        "    (\"Google contratou João em 2024.\", {\"entities\": [(0, 6, \"ORG\"), (17, 21, \"PER\")]}),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "print(\"GPU disponível? ->\", spacy.prefer_gpu())\n",
        "\n",
        "# Teste cupy diretamente:\n",
        "import cupy\n",
        "x = cupy.arange(10)\n",
        "print(\"Rodou no device:\", x.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# 2) Cria um pipeline NER do zero (pt)\n",
        "# --------------------------------------\n",
        "nlp = spacy.blank(\"en\")           # modelo em branco (sem vocabulário treinado)\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# adiciona os rótulos vistos nos dados\n",
        "for _, ann in TRAIN_DATA + DEV_DATA:\n",
        "    for start, end, label in ann.get(\"entities\", []):\n",
        "        ner.add_label(label)\n",
        "\n",
        "# helper para converter (text, ann) -> Example\n",
        "def make_examples(nlp, data):\n",
        "    examples = []\n",
        "    for text, ann in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        spans = []\n",
        "        for start, end, label in ann[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "\n",
        "            if span is None:\n",
        "                raise ValueError(f\"Span inválido em: {text!r} -> {(start, end, label)}\")\n",
        "            \n",
        "            spans.append(span)\n",
        "            \n",
        "        doc_ents = {\"entities\": [(s.start_char, s.end_char, s.label_) for s in spans]}\n",
        "        examples.append(Example.from_dict(doc, doc_ents))\n",
        "    return examples\n",
        "\n",
        "train_examples = make_examples(nlp, TRAIN_DATA)\n",
        "dev_examples   = make_examples(nlp, DEV_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# 3) Treinamento\n",
        "# --------------------------------------\n",
        "# desabilite outros pipes (aqui só temos o 'ner' mesmo)\n",
        "other_pipes = [p for p in nlp.pipe_names if p != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.initialize(get_examples=lambda: train_examples)\n",
        "    n_iter = 30\n",
        "    for itn in range(1, n_iter + 1):\n",
        "        random.shuffle(train_examples)\n",
        "        losses = {}\n",
        "        # minibatches progressivamente maiores ajudam em dados pequenos\n",
        "        for batch in minibatch(train_examples, size=4):\n",
        "            nlp.update(batch, sgd=optimizer, drop=0.2, losses=losses)\n",
        "        if itn % 5 == 0 or itn == 1 or itn == n_iter:\n",
        "            print(f\"Iter {itn:02d} - loss: {losses.get('ner', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 4 – Avaliação (corrigida)\n",
        "def evaluate(nlp, examples):\n",
        "    # gera previsões de forma vetorizada\n",
        "    pred_docs = list(nlp.pipe([ex.text for ex in examples]))\n",
        "    pred_examples = [Example(pred, ex.reference) for pred, ex in zip(pred_docs, examples)]\n",
        "\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(pred_examples)  # <-- passa a LISTA de Example\n",
        "    return {\n",
        "        \"precision\": scores[\"ents_p\"],\n",
        "        \"recall\":    scores[\"ents_r\"],\n",
        "        \"f1\":        scores[\"ents_f\"],\n",
        "    }\n",
        "\n",
        "metrics = evaluate(nlp, dev_examples)\n",
        "print(\"DEV metrics:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------\n",
        "# 5) Teste rápido\n",
        "# --------------------------------------\n",
        "tests = [\n",
        "    \"Marcos works on Apple in São Paulo.\",\n",
        "    \"the Embraer is locaded in Brasil.\",\n",
        "    \"Ana fly to new york.\",\n",
        "]\n",
        "for t in tests:\n",
        "    doc = nlp(t)\n",
        "    print(\"\\nTexto:\", t)\n",
        "    for ent in doc.ents:\n",
        "        print(f\" - {ent.text:<20} {ent.label_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 7 – Salvar o modelo treinado\n",
        "output_dir = \"modelo_ner_en\"\n",
        "nlp.to_disk(output_dir)\n",
        "print(f\"Modelo salvo em: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 8 – Carregar o modelo treinado\n",
        "import spacy\n",
        "nlp_carregado = spacy.load(\"modelo_ner_en\")\n",
        "\n",
        "# Teste rápido\n",
        "doc = nlp_carregado(\"Tom went to Paris and met with Alice.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
