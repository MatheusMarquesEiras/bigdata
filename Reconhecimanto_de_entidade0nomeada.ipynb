{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7qYGKfoMtZDR",
        "outputId": "ec419a2a-f0b7-45ca-ff00-6e3231896a5c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>through</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>London</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>protest</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>war</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Iraq</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demand</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>withdrawal</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>British</td>\n",
              "      <td>JJ</td>\n",
              "      <td>B-gpe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NaN</td>\n",
              "      <td>troops</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>NaN</td>\n",
              "      <td>from</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>NaN</td>\n",
              "      <td>that</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>NaN</td>\n",
              "      <td>country</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>NaN</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Sentence: 2</td>\n",
              "      <td>Families</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence #           Word  POS    Tag\n",
              "0   Sentence: 1      Thousands  NNS      O\n",
              "1           NaN             of   IN      O\n",
              "2           NaN  demonstrators  NNS      O\n",
              "3           NaN           have  VBP      O\n",
              "4           NaN        marched  VBN      O\n",
              "5           NaN        through   IN      O\n",
              "6           NaN         London  NNP  B-geo\n",
              "7           NaN             to   TO      O\n",
              "8           NaN        protest   VB      O\n",
              "9           NaN            the   DT      O\n",
              "10          NaN            war   NN      O\n",
              "11          NaN             in   IN      O\n",
              "12          NaN           Iraq  NNP  B-geo\n",
              "13          NaN            and   CC      O\n",
              "14          NaN         demand   VB      O\n",
              "15          NaN            the   DT      O\n",
              "16          NaN     withdrawal   NN      O\n",
              "17          NaN             of   IN      O\n",
              "18          NaN        British   JJ  B-gpe\n",
              "19          NaN         troops  NNS      O\n",
              "20          NaN           from   IN      O\n",
              "21          NaN           that   DT      O\n",
              "22          NaN        country   NN      O\n",
              "23          NaN              .    .      O\n",
              "24  Sentence: 2       Families  NNS      O"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence #: 47959\n",
            "word: 1048565\n",
            "pos: 1048575\n",
            "tag: 1048575\n"
          ]
        }
      ],
      "source": [
        "print(f\"sentence #: {data['Sentence #'].count()}\")\n",
        "print(f\"word: {data['Word'].count()}\")\n",
        "print(f\"pos: {data['POS'].count()}\")\n",
        "print(f\"tag: {data['Tag'].count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Sentence #', 'Word', 'POS', 'Tag'], dtype='object')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['NNS', 'IN', 'VBP', 'VBN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC',\n",
              "       'JJ', '.', 'VBD', 'WP', '``', 'CD', 'PRP', 'VBZ', 'POS', 'VBG',\n",
              "       'RB', ',', 'WRB', 'PRP$', 'MD', 'WDT', 'JJR', ':', 'JJS', 'WP$',\n",
              "       'RP', 'PDT', 'NNPS', 'EX', 'RBS', 'LRB', 'RRB', '$', 'RBR', ';',\n",
              "       'UH', 'FW'], dtype=object)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_all_pos = data['POS'].unique()\n",
        "data['POS'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.', '``', ',', ':', '$', ';']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_chars_unused = [item for item in list_all_pos.tolist() if not item.isupper()]\n",
        "pos_chars_unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_token2index_and_index2token(data, column):\n",
        "    vacab = list(set(data[column].to_list()))\n",
        "\n",
        "    index2token = {index:token for index, token in enumerate(vacab)}\n",
        "    token2index = {token:index for index, token in enumerate(vacab)}\n",
        "\n",
        "    return index2token, token2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2index, index2word = get_token2index_and_index2token(data=data, column='Word')\n",
        "tag2index, index2tag = get_token2index_and_index2token(data=data, column='Tag')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Word_idx'] = data['Word'].map(word2index)\n",
        "data['Tag_idx'] = data['Tag'].map(tag2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Sentence #', 'Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'], dtype='object')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Word_idx</th>\n",
              "      <th>Tag_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>through</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>London</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>protest</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>war</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Iraq</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demand</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence #           Word  POS    Tag Word_idx Tag_idx\n",
              "0   Sentence: 1      Thousands  NNS      O      NaN     NaN\n",
              "1           NaN             of   IN      O      NaN     NaN\n",
              "2           NaN  demonstrators  NNS      O      NaN     NaN\n",
              "3           NaN           have  VBP      O      NaN     NaN\n",
              "4           NaN        marched  VBN      O      NaN     NaN\n",
              "5           NaN        through   IN      O      NaN     NaN\n",
              "6           NaN         London  NNP  B-geo      NaN     NaN\n",
              "7           NaN             to   TO      O      NaN     NaN\n",
              "8           NaN        protest   VB      O      NaN     NaN\n",
              "9           NaN            the   DT      O      NaN     NaN\n",
              "10          NaN            war   NN      O      NaN     NaN\n",
              "11          NaN             in   IN      O      NaN     NaN\n",
              "12          NaN           Iraq  NNP  B-geo      NaN     NaN\n",
              "13          NaN            and   CC      O      NaN     NaN\n",
              "14          NaN         demand   VB      O      NaN     NaN"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sphVo4y7tqOj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30976\\1211054642.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data_fillna = data.ffill(axis=0)  # substitui fillna(method='ffill') metodo usado para preencher para frente\n"
          ]
        }
      ],
      "source": [
        "data_fillna = data.ffill(axis=0)  # substitui fillna(method='ffill') metodo usado para preencher para frente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Word_idx</th>\n",
              "      <th>Tag_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>through</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>London</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>protest</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>war</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Iraq</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demand</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>withdrawal</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>British</td>\n",
              "      <td>JJ</td>\n",
              "      <td>B-gpe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>troops</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>from</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>that</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>country</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Sentence: 2</td>\n",
              "      <td>Families</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence #           Word  POS    Tag  Word_idx  Tag_idx\n",
              "0   Sentence: 1      Thousands  NNS      O       NaN      NaN\n",
              "1   Sentence: 1             of   IN      O       NaN      NaN\n",
              "2   Sentence: 1  demonstrators  NNS      O       NaN      NaN\n",
              "3   Sentence: 1           have  VBP      O       NaN      NaN\n",
              "4   Sentence: 1        marched  VBN      O       NaN      NaN\n",
              "5   Sentence: 1        through   IN      O       NaN      NaN\n",
              "6   Sentence: 1         London  NNP  B-geo       NaN      NaN\n",
              "7   Sentence: 1             to   TO      O       NaN      NaN\n",
              "8   Sentence: 1        protest   VB      O       NaN      NaN\n",
              "9   Sentence: 1            the   DT      O       NaN      NaN\n",
              "10  Sentence: 1            war   NN      O       NaN      NaN\n",
              "11  Sentence: 1             in   IN      O       NaN      NaN\n",
              "12  Sentence: 1           Iraq  NNP  B-geo       NaN      NaN\n",
              "13  Sentence: 1            and   CC      O       NaN      NaN\n",
              "14  Sentence: 1         demand   VB      O       NaN      NaN\n",
              "15  Sentence: 1            the   DT      O       NaN      NaN\n",
              "16  Sentence: 1     withdrawal   NN      O       NaN      NaN\n",
              "17  Sentence: 1             of   IN      O       NaN      NaN\n",
              "18  Sentence: 1        British   JJ  B-gpe       NaN      NaN\n",
              "19  Sentence: 1         troops  NNS      O       NaN      NaN\n",
              "20  Sentence: 1           from   IN      O       NaN      NaN\n",
              "21  Sentence: 1           that   DT      O       NaN      NaN\n",
              "22  Sentence: 1        country   NN      O       NaN      NaN\n",
              "23  Sentence: 1              .    .      O       NaN      NaN\n",
              "24  Sentence: 2       Families  NNS      O       NaN      NaN"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_fillna.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Sentence: 1', 'Sentence: 2', 'Sentence: 3', ...,\n",
              "       'Sentence: 47957', 'Sentence: 47958', 'Sentence: 47959'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_fillna['Sentence #'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Retorna um Series booleano indicando quais linhas têm esse valor\n",
        "mask = data_fillna['Sentence #'] == \"Sentence: 2\"\n",
        "\n",
        "# Se quiser apenas saber se existe pelo menos um caso:\n",
        "existe = mask.any()\n",
        "\n",
        "print(existe)  # True se existir, False se não existir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coninuar aqui\n",
        "data_group = (\n",
        "    data_fillna\n",
        "      .groupby('Sentence #', as_index=False)[['Word', 'POS', 'Tag']]\n",
        "      .agg(list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group = (\n",
        "    data_fillna\n",
        "      .groupby('Sentence #', as_index=False)[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx']]\n",
        "      .agg(list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47959\n",
            "47959\n",
            "47959\n",
            "47959\n"
          ]
        }
      ],
      "source": [
        "print(data_group['Sentence #'].count())\n",
        "print(data_group['Word'].count())\n",
        "print(data_group['POS'].count())\n",
        "print(data_group['Tag'].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Retorna um Series booleano indicando quais linhas têm esse valor\n",
        "mask = data_group['Sentence #'] == \"Sentence: 2\"\n",
        "\n",
        "# Se quiser apenas saber se existe pelo menos um caso:\n",
        "existe = mask.any()\n",
        "\n",
        "print(existe)  # True se existir, False se não existir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group['Phase'] = ''\n",
        "\n",
        "# Preenche usando o loop\n",
        "for idx, row in data_group['Word'].items():\n",
        "    data_group.at[idx, 'Phase'] = ' '.join(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Thousands of demonstrators have marched throug...\n",
              "Name: Phase, dtype: object"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_group['Phase'].head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_dublespaces(text, char):\n",
        "    if f' {char} ' in text:\n",
        "        text = text.replace(f' {char} ', f'{char} ')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, row in data_group['Phase'].items():\n",
        "    for char in pos_chars_unused:\n",
        "        data_group.at[idx, 'Phase'] = remove_dublespaces(row, char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Phase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
              "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
              "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 10</td>\n",
              "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
              "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
              "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>Iranian officials say they expect to get acces...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 100</td>\n",
              "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
              "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
              "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
              "      <td>Helicopter gunships Saturday pounded militant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1000</td>\n",
              "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
              "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>They left after a tense hour-long standoff wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 10000</td>\n",
              "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
              "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
              "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
              "      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Sentence #                                               Word  \\\n",
              "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
              "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
              "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
              "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
              "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
              "\n",
              "                                                 POS  \\\n",
              "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
              "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
              "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
              "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
              "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
              "\n",
              "                                                 Tag  \\\n",
              "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
              "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
              "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
              "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
              "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
              "\n",
              "                                               Phase  \n",
              "0  Thousands of demonstrators have marched throug...  \n",
              "1  Iranian officials say they expect to get acces...  \n",
              "2  Helicopter gunships Saturday pounded militant ...  \n",
              "3  They left after a tense hour-long standoff wit...  \n",
              "4  U.N. relief coordinator Jan Egeland said Sunda...  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_group.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cria a coluna Position como lista vazia\n",
        "data_group['Position'] = [[] for _ in range(len(data_group))]\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    positions = []\n",
        "    for idxW, word in enumerate(row['Word']):\n",
        "        try:\n",
        "            index = row['Phase'].index(word)\n",
        "            lenWord = len(word)\n",
        "            positions.append((index, lenWord, row['POS'][idxW]))\n",
        "        except ValueError:\n",
        "            # Palavra não encontrada na Phase\n",
        "            pass\n",
        "    data_group.at[idx, 'Position'] = positions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Phase</th>\n",
              "      <th>Position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
              "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
              "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>[(0, 9, NNS), (10, 2, IN), (13, 13, NNS), (27,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 10</td>\n",
              "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
              "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
              "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>Iranian officials say they expect to get acces...</td>\n",
              "      <td>[(0, 7, JJ), (8, 9, NNS), (18, 3, VBP), (22, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 100</td>\n",
              "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
              "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
              "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
              "      <td>Helicopter gunships Saturday pounded militant ...</td>\n",
              "      <td>[(0, 10, NN), (11, 8, NNS), (20, 8, NNP), (29,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1000</td>\n",
              "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
              "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>They left after a tense hour-long standoff wit...</td>\n",
              "      <td>[(0, 4, PRP), (5, 4, VBD), (10, 5, IN), (10, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 10000</td>\n",
              "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
              "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
              "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
              "      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n",
              "      <td>[(0, 4, NNP), (5, 6, NN), (12, 11, NN), (24, 3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Sentence #                                               Word  \\\n",
              "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
              "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
              "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
              "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
              "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
              "\n",
              "                                                 POS  \\\n",
              "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
              "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
              "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
              "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
              "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
              "\n",
              "                                                 Tag  \\\n",
              "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
              "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
              "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
              "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
              "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
              "\n",
              "                                               Phase  \\\n",
              "0  Thousands of demonstrators have marched throug...   \n",
              "1  Iranian officials say they expect to get acces...   \n",
              "2  Helicopter gunships Saturday pounded militant ...   \n",
              "3  They left after a tense hour-long standoff wit...   \n",
              "4  U.N. relief coordinator Jan Egeland said Sunda...   \n",
              "\n",
              "                                            Position  \n",
              "0  [(0, 9, NNS), (10, 2, IN), (13, 13, NNS), (27,...  \n",
              "1  [(0, 7, JJ), (8, 9, NNS), (18, 3, VBP), (22, 4...  \n",
              "2  [(0, 10, NN), (11, 8, NNS), (20, 8, NNP), (29,...  \n",
              "3  [(0, 4, PRP), (5, 4, VBD), (10, 5, IN), (10, 1...  \n",
              "4  [(0, 4, NNP), (5, 6, NN), (12, 11, NN), (24, 3...  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_group.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# da errado mas cheguei perto\n",
        "\n",
        "data_group['Position'] = ''\n",
        "\n",
        "for idx, row in data_group.items():\n",
        "    data_group['Position'][idx] = []\n",
        "\n",
        "    for idxW, word in enumerate(data_group['Word']):\n",
        "        index = data_group['Phase'][idx].index(word)\n",
        "        lenWord = len(word)\n",
        "        data_group['Position'][idx].append((index, lenWord, data_group['POS'][idxW]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset = []\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    list_finished_dataset.append((row[\"Phase\"], {\"entities\": row[\"Position\"]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "cut = int(len(list_finished_dataset) * 0.1)\n",
        "\n",
        "DEV_DATA = list_finished_dataset[:cut]\n",
        "TRAIN_DATA = list_finished_dataset[cut:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A64VDWS6uLHg",
        "outputId": "0b6779c5-b3ce-468c-9906-b65e989e7179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_tokens length: 32372 \n",
            "train_tokens length: 32372 \n",
            "test_tokens length: 4796 \n",
            "test_tags: 4796 \n",
            "val_tokens: 10791 \n",
            "val_tags: 10791\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "def get_pad_train_test_val(data_group, data):\n",
        "\n",
        "    #get max token and tag length\n",
        "    n_token = len(list(set(data['Word'].to_list())))\n",
        "    n_tag = len(list(set(data['Tag'].to_list())))\n",
        "\n",
        "    #Pad tokens (X var)\n",
        "    tokens = data_group['Word_idx'].tolist()\n",
        "    maxlen = max([len(s) for s in tokens])\n",
        "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
        "\n",
        "    #Pad Tags (y var) and convert it into one hot encoding\n",
        "    tags = data_group['Tag_idx'].tolist()\n",
        "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
        "    n_tags = len(tag2idx)\n",
        "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
        "\n",
        "    #Split train, test and validation set\n",
        "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
        "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
        "\n",
        "    print(\n",
        "        'train_tokens length:', len(train_tokens),\n",
        "        '\\ntrain_tokens length:', len(train_tokens),\n",
        "        '\\ntest_tokens length:', len(test_tokens),\n",
        "        '\\ntest_tags:', len(test_tags),\n",
        "        '\\nval_tokens:', len(val_tokens),\n",
        "        '\\nval_tags:', len(val_tags),\n",
        "    )\n",
        "\n",
        "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
        "\n",
        "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK68wMhHzU0D",
        "outputId": "27b73cb9-1d43-4266-e9e1-c9b68d23edbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_tokens length: 32372 \n",
            "train_tokens length: 32372 \n",
            "test_tokens length: 4796 \n",
            "test_tags: 4796 \n",
            "val_tokens: 10791 \n",
            "val_tags: 10791\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "def get_pad_train_test_val(data_group, data):\n",
        "\n",
        "    #get max token and tag length\n",
        "    n_token = len(list(set(data['Word'].to_list())))\n",
        "    n_tag = len(list(set(data['Tag'].to_list())))\n",
        "\n",
        "    #Pad tokens (X var)\n",
        "    tokens = data_group['Word_idx'].tolist()\n",
        "    maxlen = max([len(s) for s in tokens])\n",
        "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
        "\n",
        "    #Pad Tags (y var) and convert it into one hot encoding\n",
        "    tags = data_group['Tag_idx'].tolist()\n",
        "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
        "    n_tags = len(tag2idx)\n",
        "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
        "\n",
        "    #Split train, test and validation set\n",
        "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
        "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
        "\n",
        "    print(\n",
        "        'train_tokens length:', len(train_tokens),\n",
        "        '\\ntrain_tokens length:', len(train_tokens),\n",
        "        '\\ntest_tokens length:', len(test_tokens),\n",
        "        '\\ntest_tags:', len(test_tags),\n",
        "        '\\nval_tokens:', len(val_tokens),\n",
        "        '\\nval_tags:', len(val_tags),\n",
        "    )\n",
        "\n",
        "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
        "\n",
        "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_X66OzNWzZ8T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras import Sequential, Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tensorflow.random.set_seed(2)\n",
        "\n",
        "input_dim = len(list(set(data['Word'].to_list())))+1\n",
        "output_dim = 64\n",
        "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
        "n_tags = len(tag2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y4oa_QR-zgrs"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Embedding layer\n",
        "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
        "\n",
        "    # Add bidirectional LSTM\n",
        "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
        "\n",
        "    # Add LSTM\n",
        "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
        "\n",
        "    #Optimiser\n",
        "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "lQTfmRUDzgp0",
        "outputId": "d4f467e2-423c-43e0-fb6c-9eb0e49aba6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 3s/step - accuracy: 0.8038 - loss: 2.2385 - val_accuracy: 0.9681 - val_loss: 0.4238\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.9676 - loss: 0.3937 - val_accuracy: 0.9681 - val_loss: 0.3002\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.9676 - loss: 0.3328 - val_accuracy: 0.9681 - val_loss: 0.2683\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.9675 - loss: 0.3028 - val_accuracy: 0.9681 - val_loss: 0.2385\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.9676 - loss: 0.2755 - val_accuracy: 0.9681 - val_loss: 0.2256\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9677 - loss: 0.2513 - val_accuracy: 0.9682 - val_loss: 0.2001\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.9677 - loss: 0.2267 - val_accuracy: 0.9682 - val_loss: 0.1909\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.9677 - loss: 0.2127 - val_accuracy: 0.9683 - val_loss: 0.2130\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9678 - loss: 0.2352 - val_accuracy: 0.9683 - val_loss: 0.1832\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3s/step - accuracy: 0.9678 - loss: 0.2027 - val_accuracy: 0.9683 - val_loss: 0.1671\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9678 - loss: 0.1886 - val_accuracy: 0.9683 - val_loss: 0.1723\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.9679 - loss: 0.1836 - val_accuracy: 0.9684 - val_loss: 0.1481\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.9679 - loss: 0.1656 - val_accuracy: 0.9687 - val_loss: 0.1467\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9682 - loss: 0.1625 - val_accuracy: 0.9687 - val_loss: 0.1359\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.9682 - loss: 0.1498 - val_accuracy: 0.9689 - val_loss: 0.1318\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.9683 - loss: 0.1451 - val_accuracy: 0.9690 - val_loss: 0.1278\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9683 - loss: 0.1585 - val_accuracy: 0.9690 - val_loss: 0.1337\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.9684 - loss: 0.1432 - val_accuracy: 0.9690 - val_loss: 0.1256\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.9684 - loss: 0.1343 - val_accuracy: 0.9692 - val_loss: 0.1240\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3s/step - accuracy: 0.9686 - loss: 0.1295 - val_accuracy: 0.9692 - val_loss: 0.1226\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.9687 - loss: 0.1259 - val_accuracy: 0.9693 - val_loss: 0.1196\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 3s/step - accuracy: 0.9689 - loss: 0.1226 - val_accuracy: 0.9696 - val_loss: 0.1213\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 3s/step - accuracy: 0.9580 - loss: 0.1950 - val_accuracy: 0.9687 - val_loss: 0.1514\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.9679 - loss: 0.1587 - val_accuracy: 0.9687 - val_loss: 0.1287\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.9682 - loss: 0.1419 - val_accuracy: 0.9688 - val_loss: 0.1267\n"
          ]
        }
      ],
      "source": [
        "def train_model(X, y, model):\n",
        "    loss = list()\n",
        "    for i in range(25):\n",
        "        # fit model for one epoch on this sequence\n",
        "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
        "        loss.append(hist.history['loss'][0])\n",
        "    return loss\n",
        "\n",
        "results = pd.DataFrame()\n",
        "model_bilstm_lstm = get_bilstm_lstm_model()\n",
        "model_bilstm_lstm.build(input_shape=(None, train_tokens.shape[1])) # Explicitly build the model\n",
        "plot_model(model_bilstm_lstm)\n",
        "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Q09ZsER2zgnd",
        "outputId": "dc3e8027-24ca-4be8-abbf-e0c79435ea8b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hi, My name is \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Aman Kharwal \n",
              " \n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "I am from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " <br> I want to work with \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google \n",
              " Steve Jobs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is My Inspiration</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = nlp('Hi, My name is Aman Kharwal \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration')\n",
        "displacy.render(text, style = 'ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U spacy>=3.7\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1) Dados mock (corrigidos)\n",
        "# -----------------------------\n",
        "TRAIN_DATA = [\n",
        "    (\"João mora em São Paulo.\", {\"entities\": [(0, 4, \"PER\"), (13, 22, \"LOC\")]}),\n",
        "    (\"Maria trabalha na Google em Belo Horizonte.\", {\"entities\": [(0, 5, \"PER\"), (18, 24, \"ORG\"), (28, 42, \"LOC\")]}),\n",
        "    (\"A Apple comprou a Embraer.\", {\"entities\": [(2, 7, \"ORG\"), (18, 25, \"ORG\")]}),\n",
        "    (\"A Ana visitou o Rio de Janeiro.\", {\"entities\": [(2, 5, \"PER\"), (16, 30, \"LOC\")]}),\n",
        "    (\"Pedro nasceu em Lisboa.\", {\"entities\": [(0, 5, \"PER\"), (16, 22, \"LOC\")]}),\n",
        "    (\"A Microsoft abriu escritório em Recife.\", {\"entities\": [(2, 11, \"ORG\"), (32, 38, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "DEV_DATA = [\n",
        "    (\"Carla foi para Porto Alegre ontem.\", {\"entities\": [(0, 5, \"PER\"), (15, 27, \"LOC\")]}),\n",
        "    (\"Google contratou João em 2024.\", {\"entities\": [(0, 6, \"ORG\"), (17, 21, \"PER\")]}),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Span inválido em: 'A purported Taleban commander says al-Qaida terror network chief Osama bin Laden and Taleban leader Mullah Mohammad Omar are both alive after more than three years on the run .' -> (2, 9, 'JJ')",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[78], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         examples\u001b[38;5;241m.\u001b[39mappend(Example\u001b[38;5;241m.\u001b[39mfrom_dict(doc, doc_ents))\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n\u001b[1;32m---> 30\u001b[0m train_examples \u001b[38;5;241m=\u001b[39m \u001b[43mmake_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_DATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m dev_examples   \u001b[38;5;241m=\u001b[39m make_examples(nlp, DEV_DATA)\n",
            "Cell \u001b[1;32mIn[78], line 22\u001b[0m, in \u001b[0;36mmake_examples\u001b[1;34m(nlp, data)\u001b[0m\n\u001b[0;32m     19\u001b[0m     span \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mchar_span(start, end, label\u001b[38;5;241m=\u001b[39mlabel, alignment_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontract\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpan inválido em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(start,\u001b[38;5;250m \u001b[39mend,\u001b[38;5;250m \u001b[39mlabel)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     spans\u001b[38;5;241m.\u001b[39mappend(span)\n\u001b[0;32m     26\u001b[0m doc_ents \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(s\u001b[38;5;241m.\u001b[39mstart_char, s\u001b[38;5;241m.\u001b[39mend_char, s\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spans]}\n",
            "\u001b[1;31mValueError\u001b[0m: Span inválido em: 'A purported Taleban commander says al-Qaida terror network chief Osama bin Laden and Taleban leader Mullah Mohammad Omar are both alive after more than three years on the run .' -> (2, 9, 'JJ')"
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# 2) Cria um pipeline NER do zero (pt)\n",
        "# --------------------------------------\n",
        "nlp = spacy.blank(\"en\")           # modelo em branco (sem vocabulário treinado)\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# adiciona os rótulos vistos nos dados\n",
        "for _, ann in TRAIN_DATA + DEV_DATA:\n",
        "    for start, end, label in ann.get(\"entities\", []):\n",
        "        ner.add_label(label)\n",
        "\n",
        "# helper para converter (text, ann) -> Example\n",
        "def make_examples(nlp, data):\n",
        "    examples = []\n",
        "    for text, ann in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        spans = []\n",
        "        for start, end, label in ann[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "\n",
        "            if span is None:\n",
        "                raise ValueError(f\"Span inválido em: {text!r} -> {(start, end, label)}\")\n",
        "            \n",
        "            spans.append(span)\n",
        "            \n",
        "        doc_ents = {\"entities\": [(s.start_char, s.end_char, s.label_) for s in spans]}\n",
        "        examples.append(Example.from_dict(doc, doc_ents))\n",
        "    return examples\n",
        "\n",
        "train_examples = make_examples(nlp, TRAIN_DATA)\n",
        "dev_examples   = make_examples(nlp, DEV_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'doc_annotation': {'cats': {}, 'entities': ['U-PER', 'O', 'O', 'B-LOC', 'L-LOC', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['João', 'mora', 'em', 'São', 'Paulo', '.'], 'SPACY': [True, True, True, True, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['U-PER', 'O', 'O', 'U-ORG', 'O', 'B-LOC', 'L-LOC', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Maria', 'trabalha', 'na', 'Google', 'em', 'Belo', 'Horizonte', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['O', 'U-ORG', 'O', 'O', 'U-ORG', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'Apple', 'comprou', 'a', 'Embraer', '.'], 'SPACY': [True, True, True, True, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['O', 'U-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'L-LOC', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'Ana', 'visitou', 'o', 'Rio', 'de', 'Janeiro', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['U-PER', 'O', 'O', 'U-LOC', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Pedro', 'nasceu', 'em', 'Lisboa', '.'], 'SPACY': [True, True, True, False, False], 'TAG': ['', '', '', '', ''], 'LEMMA': ['', '', '', '', ''], 'POS': ['', '', '', '', ''], 'MORPH': ['', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4], 'DEP': ['', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['O', 'U-ORG', 'O', 'O', 'O', 'U-LOC', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'Microsoft', 'abriu', 'escritório', 'em', 'Recife', '.'], 'SPACY': [True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6], 'DEP': ['', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0]}}]\n",
            "<class 'list'>\n",
            "[{'doc_annotation': {'cats': {}, 'entities': ['U-PER', 'O', 'O', 'B-LOC', 'L-LOC', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Carla', 'foi', 'para', 'Porto', 'Alegre', 'ontem', '.'], 'SPACY': [True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6], 'DEP': ['', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0]}}, {'doc_annotation': {'cats': {}, 'entities': ['U-ORG', 'O', 'U-PER', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Google', 'contratou', 'João', 'em', '2024', '.'], 'SPACY': [True, True, True, True, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}]\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(train_examples)\n",
        "print(type(train_examples))\n",
        "print(dev_examples)\n",
        "print(type(dev_examples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 01 - loss: 30.0779\n",
            "Iter 05 - loss: 29.0852\n",
            "Iter 10 - loss: 15.5622\n",
            "Iter 15 - loss: 11.8380\n",
            "Iter 20 - loss: 5.2793\n",
            "Iter 25 - loss: 0.8475\n",
            "Iter 30 - loss: 0.0020\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# 3) Treinamento\n",
        "# --------------------------------------\n",
        "# desabilite outros pipes (aqui só temos o 'ner' mesmo)\n",
        "other_pipes = [p for p in nlp.pipe_names if p != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.initialize(get_examples=lambda: train_examples)\n",
        "    n_iter = 30\n",
        "    for itn in range(1, n_iter + 1):\n",
        "        random.shuffle(train_examples)\n",
        "        losses = {}\n",
        "        # minibatches progressivamente maiores ajudam em dados pequenos\n",
        "        for batch in minibatch(train_examples, size=4):\n",
        "            nlp.update(batch, sgd=optimizer, drop=0.2, losses=losses)\n",
        "        if itn % 5 == 0 or itn == 1 or itn == n_iter:\n",
        "            print(f\"Iter {itn:02d} - loss: {losses.get('ner', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEV metrics: {'precision': 0.5, 'recall': 0.5, 'f1': 0.5}\n"
          ]
        }
      ],
      "source": [
        "# 📌 Célula 4 – Avaliação (corrigida)\n",
        "def evaluate(nlp, examples):\n",
        "    # gera previsões de forma vetorizada\n",
        "    pred_docs = list(nlp.pipe([ex.text for ex in examples]))\n",
        "    pred_examples = [Example(pred, ex.reference) for pred, ex in zip(pred_docs, examples)]\n",
        "\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(pred_examples)  # <-- passa a LISTA de Example\n",
        "    return {\n",
        "        \"precision\": scores[\"ents_p\"],\n",
        "        \"recall\":    scores[\"ents_r\"],\n",
        "        \"f1\":        scores[\"ents_f\"],\n",
        "    }\n",
        "\n",
        "metrics = evaluate(nlp, dev_examples)\n",
        "print(\"DEV metrics:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Texto: Marcos trabalha na Apple em São Paulo.\n",
            " - Marcos               PER\n",
            " - Apple                ORG\n",
            " - São Paulo            LOC\n",
            "\n",
            "Texto: A Embraer fica no Brasil.\n",
            " - Embraer              ORG\n",
            " - Brasil               ORG\n",
            "\n",
            "Texto: A Ana voou para Porto Alegre.\n",
            " - Ana                  PER\n",
            " - Porto                ORG\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------------------------------------\n",
        "# 5) Teste rápido\n",
        "# --------------------------------------\n",
        "tests = [\n",
        "    \"Marcos trabalha na Apple em São Paulo.\",\n",
        "    \"A Embraer fica no Brasil.\",\n",
        "    \"A Ana voou para Porto Alegre.\",\n",
        "]\n",
        "for t in tests:\n",
        "    doc = nlp(t)\n",
        "    print(\"\\nTexto:\", t)\n",
        "    for ent in doc.ents:\n",
        "        print(f\" - {ent.text:<20} {ent.label_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo salvo em: modelo_ner_pt\n"
          ]
        }
      ],
      "source": [
        "# 📌 Célula 7 – Salvar o modelo treinado\n",
        "output_dir = \"modelo_ner_pt\"\n",
        "nlp.to_disk(output_dir)\n",
        "print(f\"Modelo salvo em: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ana PER\n",
            "Lisboa LOC\n"
          ]
        }
      ],
      "source": [
        "# 📌 Célula 8 – Carregar o modelo treinado\n",
        "import spacy\n",
        "nlp_carregado = spacy.load(\"modelo_ner_pt\")\n",
        "\n",
        "# Teste rápido\n",
        "doc = nlp_carregado(\"Ana foi para Lisboa.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
