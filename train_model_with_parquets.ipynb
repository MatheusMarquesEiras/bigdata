{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from enum import Enum\n",
        "import re\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "prqt = pd.read_parquet('./parquet/MultL/test_multilingual.parquet', engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['tokens', 'ner_tags', 'lang'], dtype='object')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prqt.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def corrigir_espacos(texto: str) -> str:\n",
        "    \"\"\"\n",
        "    Corrige espaçamento incorreto em frases de português/espanhol/inglês.\n",
        "    Regras básicas:\n",
        "    - Nenhum espaço antes de pontuação (. , ; : ? ! …)\n",
        "    - Um espaço depois de pontuação, exceto antes de fechar aspas/parênteses\n",
        "    - Espaço antes de abrir parênteses/colchetes/chaves\n",
        "    - Nenhum espaço depois de abrir ou antes de fechar parênteses/colchetes/chaves\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove espaços antes de sinais de pontuação\n",
        "    texto = re.sub(r'\\s+([.,;:!?…])', r'\\1', texto)\n",
        "\n",
        "    # Garante um espaço depois de sinais de pontuação (se não for fim de frase ou antes de aspas/fechamentos)\n",
        "    texto = re.sub(r'([.,;:!?…])([^\\s\"\\'\\)\\]\\}])', r'\\1 \\2', texto)\n",
        "\n",
        "    # Espaço antes de abrir parênteses/colchetes/chaves, se não houver\n",
        "    texto = re.sub(r'(\\w)([\\(\\[\\{])', r'\\1 \\2', texto)\n",
        "\n",
        "    # Remove espaço logo após abrir parênteses/colchetes/chaves\n",
        "    texto = re.sub(r'([\\(\\[\\{])\\s+', r'\\1', texto)\n",
        "\n",
        "    # Remove espaço logo antes de fechar parênteses/colchetes/chaves\n",
        "    texto = re.sub(r'\\s+([\\)\\]\\}])', r'\\1', texto)\n",
        "\n",
        "    # Aspas: remove espaço depois de abrir aspas\n",
        "    texto = re.sub(r'([“\"\\'«])\\s+', r'\\1', texto)\n",
        "\n",
        "    # Aspas: remove espaço antes de fechar aspas\n",
        "    texto = re.sub(r'\\s+([”\"\\'»])', r'\\1', texto)\n",
        "\n",
        "    # Espaço depois de fechar aspas, se seguido de palavra\n",
        "    texto = re.sub(r'([”\"\\'»])([A-Za-zÁ-ú])', r'\\1 \\2', texto)\n",
        "\n",
        "    return texto\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "prqt['phase'] = prqt['tokens'].apply(lambda x: corrigir_espacos(\" \".join(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "prqt['dict'] = prqt.apply(\n",
        "    lambda row: dict(zip(row['tokens'], row['ner_tags'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "prqt['tup'] = prqt.apply(\n",
        "    lambda row: (row['tokens'], row['ner_tags']),\n",
        "    axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classificação conforme os intervalos\n",
        "intervalos = [\n",
        "    (range(1, 3), \"PER\"),\n",
        "    (range(3, 5), \"ORG\"),\n",
        "    (range(5, 7), \"LOC\"),\n",
        "    (range(7, 9), \"MIST\"),\n",
        "]\n",
        "\n",
        "def classificar(valor: int) -> str:\n",
        "    for intervalo, significado in intervalos:\n",
        "        if valor in intervalo:\n",
        "            return significado\n",
        "    return None\n",
        "\n",
        "# Função principal que opera sobre a tupla (tokens, tags)\n",
        "def processar_tupla(tupla):\n",
        "    tokens, tags = tupla\n",
        "    resultado = []\n",
        "    grupo = []\n",
        "    label_atual = None\n",
        "\n",
        "    for palavra, valor in zip(tokens, tags):\n",
        "        if valor == 0:\n",
        "            # Fecha o grupo se tiver algo\n",
        "            if grupo:\n",
        "                frase = corrigir_espacos(\" \".join(grupo))\n",
        "                resultado.append((frase, label_atual))\n",
        "                grupo = []\n",
        "                label_atual = None\n",
        "            continue\n",
        "        \n",
        "        if valor % 2 == 1:\n",
        "            # Novo grupo\n",
        "            if grupo:\n",
        "                frase = corrigir_espacos(\" \".join(grupo))\n",
        "                resultado.append((frase, label_atual))\n",
        "            grupo = [palavra]\n",
        "            label_atual = classificar(valor)\n",
        "        \n",
        "        elif valor % 2 == 0 and grupo:\n",
        "            grupo.append(palavra)\n",
        "\n",
        "    # Fecha o último grupo\n",
        "    if grupo:\n",
        "        frase = corrigir_espacos(\" \".join(grupo))\n",
        "        resultado.append((frase, label_atual))\n",
        "    \n",
        "    return resultado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "prqt[\"substring_labeled\"] = prqt[\"tup\"].apply(processar_tupla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_position(text: str, sub: str) -> tuple | None:\n",
        "    begin = text.find(sub)\n",
        "    if begin == -1:\n",
        "        return None\n",
        "    end = begin + len(sub)\n",
        "    return begin, end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "positions = []\n",
        "\n",
        "# Itera sobre as linhas do DataFrame\n",
        "for _, row in prqt.iterrows():\n",
        "    l = []\n",
        "    for sub in row[\"substring_labeled\"]:\n",
        "        texto_entidade = sub[0]   # a frase extraída\n",
        "        label = sub[1]            # o label: \"PER\", \"ORG\", etc.\n",
        "        begin, end = get_position(text=row[\"phase\"], sub=texto_entidade)\n",
        "        l.append((begin, end, label))\n",
        "    positions.append(l)\n",
        "\n",
        "# Adiciona a nova coluna ao DataFrame\n",
        "prqt[\"position\"] = positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Linhas com pelo menos um (None, None): 0\n",
            "✅ Linhas com todos os offsets válidos: 31375\n"
          ]
        }
      ],
      "source": [
        "def tem_none(posicoes):\n",
        "    return any(pos[0] is None or pos[1] is None for pos in posicoes)\n",
        "\n",
        "# Cria uma coluna booleana temporária\n",
        "prqt[\"has_none\"] = prqt[\"position\"].apply(tem_none)\n",
        "\n",
        "# Conta as linhas\n",
        "linhas_com_none = prqt[\"has_none\"].sum()\n",
        "linhas_sem_none = len(prqt) - linhas_com_none\n",
        "\n",
        "print(f\"❌ Linhas com pelo menos um (None, None): {linhas_com_none}\")\n",
        "print(f\"✅ Linhas com todos os offsets válidos: {linhas_sem_none}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset = []\n",
        "\n",
        "for idx, row in prqt.iterrows():\n",
        "    list_finished_dataset.append((row[\"phase\"], {\"entities\": row[\"position\"]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "cut = int(len(list_finished_dataset) * 0.1)\n",
        "\n",
        "DEV_DATA = list_finished_dataset[:cut]\n",
        "TRAIN_DATA = list_finished_dataset[cut:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1) Dados mock (corrigidos)\n",
        "# -----------------------------\n",
        "TRAIN_DATA = [\n",
        "    (\"João mora em São Paulo.\", {\"entities\": [(0, 4, \"PER\"), (13, 22, \"LOC\")]}),\n",
        "    (\"Maria trabalha na Google em Belo Horizonte.\", {\"entities\": [(0, 5, \"PER\"), (18, 24, \"ORG\"), (28, 42, \"LOC\")]}),\n",
        "    (\"A Apple comprou a Embraer.\", {\"entities\": [(2, 7, \"ORG\"), (18, 25, \"ORG\")]}),\n",
        "    (\"A Ana visitou o Rio de Janeiro.\", {\"entities\": [(2, 5, \"PER\"), (16, 30, \"LOC\")]}),\n",
        "    (\"Pedro nasceu em Lisboa.\", {\"entities\": [(0, 5, \"PER\"), (16, 22, \"LOC\")]}),\n",
        "    (\"A Microsoft abriu escritório em Recife.\", {\"entities\": [(2, 11, \"ORG\"), (32, 38, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "DEV_DATA = [\n",
        "    (\"Carla foi para Porto Alegre ontem.\", {\"entities\": [(0, 5, \"PER\"), (15, 27, \"LOC\")]}),\n",
        "    (\"Google contratou João em 2024.\", {\"entities\": [(0, 6, \"ORG\"), (17, 21, \"PER\")]}),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Span inválido em: 'The lake also has an endemic species flock of amphipods consisting of 11\" Hyalella\"(an additional Titicaca\" Hyalella\" species is nonendemic).' -> (74, 82, 'LOC')",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[85], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         examples\u001b[38;5;241m.\u001b[39mappend(Example\u001b[38;5;241m.\u001b[39mfrom_dict(doc, doc_ents))\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n\u001b[1;32m---> 30\u001b[0m train_examples \u001b[38;5;241m=\u001b[39m \u001b[43mmake_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_DATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m dev_examples   \u001b[38;5;241m=\u001b[39m make_examples(nlp, DEV_DATA)\n",
            "Cell \u001b[1;32mIn[85], line 22\u001b[0m, in \u001b[0;36mmake_examples\u001b[1;34m(nlp, data)\u001b[0m\n\u001b[0;32m     19\u001b[0m     span \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mchar_span(start, end, label\u001b[38;5;241m=\u001b[39mlabel, alignment_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontract\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpan inválido em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(start,\u001b[38;5;250m \u001b[39mend,\u001b[38;5;250m \u001b[39mlabel)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     spans\u001b[38;5;241m.\u001b[39mappend(span)\n\u001b[0;32m     26\u001b[0m doc_ents \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(s\u001b[38;5;241m.\u001b[39mstart_char, s\u001b[38;5;241m.\u001b[39mend_char, s\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spans]}\n",
            "\u001b[1;31mValueError\u001b[0m: Span inválido em: 'The lake also has an endemic species flock of amphipods consisting of 11\" Hyalella\"(an additional Titicaca\" Hyalella\" species is nonendemic).' -> (74, 82, 'LOC')"
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# 2) Cria um pipeline NER do zero (pt)\n",
        "# --------------------------------------\n",
        "nlp = spacy.blank(\"pt\")           # modelo em branco (sem vocabulário treinado)\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# adiciona os rótulos vistos nos dados\n",
        "for _, ann in TRAIN_DATA + DEV_DATA:\n",
        "    for start, end, label in ann.get(\"entities\", []):\n",
        "        ner.add_label(label)\n",
        "\n",
        "# helper para converter (text, ann) -> Example\n",
        "def make_examples(nlp, data):\n",
        "    examples = []\n",
        "    for text, ann in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        spans = []\n",
        "        for start, end, label in ann[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "\n",
        "            if span is None:\n",
        "                raise ValueError(f\"Span inválido em: {text!r} -> {(start, end, label)}\")\n",
        "            \n",
        "            spans.append(span)\n",
        "            \n",
        "        doc_ents = {\"entities\": [(s.start_char, s.end_char, s.label_) for s in spans]}\n",
        "        examples.append(Example.from_dict(doc, doc_ents))\n",
        "    return examples\n",
        "\n",
        "train_examples = make_examples(nlp, TRAIN_DATA)\n",
        "dev_examples   = make_examples(nlp, DEV_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# 3) Treinamento\n",
        "# --------------------------------------\n",
        "# desabilite outros pipes (aqui só temos o 'ner' mesmo)\n",
        "other_pipes = [p for p in nlp.pipe_names if p != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.initialize(get_examples=lambda: train_examples)\n",
        "    n_iter = 30\n",
        "    for itn in range(1, n_iter + 1):\n",
        "        random.shuffle(train_examples)\n",
        "        losses = {}\n",
        "        # minibatches progressivamente maiores ajudam em dados pequenos\n",
        "        for batch in minibatch(train_examples, size=4):\n",
        "            nlp.update(batch, sgd=optimizer, drop=0.2, losses=losses)\n",
        "        if itn % 5 == 0 or itn == 1 or itn == n_iter:\n",
        "            print(f\"Iter {itn:02d} - loss: {losses.get('ner', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 4 – Avaliação (corrigida)\n",
        "def evaluate(nlp, examples):\n",
        "    # gera previsões de forma vetorizada\n",
        "    pred_docs = list(nlp.pipe([ex.text for ex in examples]))\n",
        "    pred_examples = [Example(pred, ex.reference) for pred, ex in zip(pred_docs, examples)]\n",
        "\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(pred_examples)  # <-- passa a LISTA de Example\n",
        "    return {\n",
        "        \"precision\": scores[\"ents_p\"],\n",
        "        \"recall\":    scores[\"ents_r\"],\n",
        "        \"f1\":        scores[\"ents_f\"],\n",
        "    }\n",
        "\n",
        "metrics = evaluate(nlp, dev_examples)\n",
        "print(\"DEV metrics:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------\n",
        "# 5) Teste rápido\n",
        "# --------------------------------------\n",
        "tests = [\n",
        "    \"Marcos works on Apple in São Paulo.\",\n",
        "    \"the Embraer is locaded in Brasil.\",\n",
        "    \"Ana fly to new york.\",\n",
        "]\n",
        "for t in tests:\n",
        "    doc = nlp(t)\n",
        "    print(\"\\nTexto:\", t)\n",
        "    for ent in doc.ents:\n",
        "        print(f\" - {ent.text:<20} {ent.label_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 7 – Salvar o modelo treinado\n",
        "output_dir = \"modelo_ner_en\"\n",
        "nlp.to_disk(output_dir)\n",
        "print(f\"Modelo salvo em: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📌 Célula 8 – Carregar o modelo treinado\n",
        "import spacy\n",
        "nlp_carregado = spacy.load(\"modelo_ner_en\")\n",
        "\n",
        "# Teste rápido\n",
        "doc = nlp_carregado(\"Tom went to Paris and met with Alice.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
