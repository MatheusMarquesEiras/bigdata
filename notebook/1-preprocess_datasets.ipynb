{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0476fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe800c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en', 'es', 'pt']\n",
    "data_types = ['test', 'val', 'train']\n",
    "datasets = ['HAREM', 'LeNER-Br', 'MultL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fcb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_list_MultL(text: str):\n",
    "    return ast.literal_eval(text.replace(' ', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01be6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_list(text: str) -> str:\n",
    "    text = re.sub(r'\\[\\s+', '[', text)\n",
    "    text = re.sub(r'\\s+\\]', ']', text)\n",
    "    text = re.sub(r'\\s+', ',', text)\n",
    "    return ast.literal_eval(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7199ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ellipsis(x):\n",
    "    if isinstance(x, list):\n",
    "        return [\"\" if item is Ellipsis else item for item in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e6c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets[2:]:\n",
    "    for lang in langs:\n",
    "        for typ in data_types: \n",
    "            en = pd.read_csv(f'../data/csv/{dataset}/{typ}_{lang}.csv')\n",
    "            es = pd.read_csv(f'../data/csv/{dataset}/{typ}_{lang}.csv')\n",
    "            pt = pd.read_csv(f'../data/csv/{dataset}/{typ}_{lang}.csv')\n",
    "\n",
    "            multL = pd.concat([en, es, pt], ignore_index=True)\n",
    "\n",
    "            multL['tokens'] = multL['tokens'].apply(fix_list_MultL)\n",
    "            multL['ner_tags'] = multL['ner_tags'].apply(fix_list_MultL)\n",
    "\n",
    "            multL = multL[['tokens', 'ner_tags']]\n",
    "\n",
    "            multL.to_parquet(f'../data/parquet/{dataset}/{typ}_{lang}_{dataset}.parquet', engine='fastparquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144de015",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmultL\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multL' is not defined"
     ]
    }
   ],
   "source": [
    "multL.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [W, ., JAMES, Willian, James, Willian, James, ...   \n",
      "1                  [Um, pouco, de, , Barrosão, \", .]   \n",
      "2  [-, Não, faço, mistério, disso, ;, mora, com, ...   \n",
      "3  [CLIPSEGURO, -, o, clipping, mais, SEGURO, da,...   \n",
      "4  [Qual, é, o, seu, nome, ?, Ventura, Pires, da,...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [1, 2, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1                              [0, 0, 0, , 12, 0, 0]  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [15, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "4  [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, ...  \n",
      "                                              tokens  \\\n",
      "0  [Fernando, Ferreira, [, click, for, a, page, i...   \n",
      "1  [BOMBEIROS, VOLUNTÁRIOS, DE, VILA, NOVA, DE, O...   \n",
      "2  [Sunab, autua, empresas, por, alta, abusiva, d...   \n",
      "3  [A, REVISTA, SÃO, PAULO, EM, PERSPECTIVA, ,, d...   \n",
      "4  [Concurso, Para, Auditor, Fiscal, do, INSS, Já...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 4, 4, ...  \n",
      "1  [3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 3, ...  \n",
      "3  [0, 0, 17, 18, 18, 18, 0, 0, 3, 4, 0, 5, 6, 6,...  \n",
      "4  [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "                                              tokens  \\\n",
      "0  [Abraço, Página, Principal, ASSOCIAÇÃO, DE, AP...   \n",
      "1  [HISTÓRICO, Esta, seção, traz, de, volta, um, ...   \n",
      "2  [C, o, m, p, r, a, s, ., ., ., COMPRAS, As, me...   \n",
      "3  [Empates, na, Honra, O, Ovarense, -, Amora, e,...   \n",
      "4  [Lula, declarou, admirar, Hitler, e, Khomeini,...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [3, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 13, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,...  \n",
      "4  [1, 0, 0, 1, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...  \n",
      "                                              tokens  \\\n",
      "0  [Número, do, Acórdão, ACÓRDÃO, 1160/2016, -, P...   \n",
      "1  [Interessado, /, Responsável, /, Recorrente, 3...   \n",
      "2  [Interessados/Responsáveis/Recorrentes, :, 3.1...   \n",
      "3  [Interessado, :, Superior, Tribunal, Militar, ...   \n",
      "4  [Responsáveis, :, Aldo, da, Silva, Fagundes, (...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [0, 0, 0, 11, 12, 0, 1, 0, 3, 4, 11, 12, 0, 0,...  \n",
      "1                              [0, 0, 0, 0, 0, 0, 0]  \n",
      "2                                       [0, 0, 0, 0]  \n",
      "3                     [0, 0, 1, 2, 2, 0, 0, 0, 0, 0]  \n",
      "4  [0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 3, 4, 4, 4, 0, ...  \n",
      "                                              tokens  \\\n",
      "0  [E, M, E, N, T, A, Órgão, :, 8ª, TURMA, CÍVEL,...   \n",
      "1                               [AÇÃO, MONITÓRIA, .]   \n",
      "2  [CITAÇÃO, REALIZADA, APÓS, O, DECURSO, DO, PRA...   \n",
      "3          [PRESCRIÇÃO, DA, PRETENSÃO, MONITÓRIA, .]   \n",
      "4                                [RECONHECIMENTO, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, ...  \n",
      "1                                          [0, 0, 0]  \n",
      "2                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3                                    [0, 0, 0, 0, 0]  \n",
      "4                                             [0, 0]  \n",
      "                                              tokens  \\\n",
      "0  [EMENTA, :, APELAÇÃO, CÍVEL, -, AÇÃO, DE, INDE...   \n",
      "1  [-, O, art, ., 178, ,, II, ,, do, CPC, prescre...   \n",
      "2  [-, Tratando-se, de, ação, indenizatória, ajui...   \n",
      "3  [-, Tendo, o, vício, sido, arguido, pelo, Parq...   \n",
      "4  [-, Preliminar, acolhida, para, reconhecer, a,...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 9, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0,...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets[:2]:\n",
    "    for lang in langs[2:]:\n",
    "        for typ in data_types:\n",
    "            data = pd.read_csv(f'../data/csv/{dataset}/{typ}.csv')\n",
    "\n",
    "            data['tokens'] = data['tokens'].apply(fix_list)\n",
    "            data['tokens'] = data['tokens'].apply(clean_ellipsis)\n",
    "\n",
    "            data['ner_tags'] = data['ner_tags'].apply(fix_list)\n",
    "            data['ner_tags'] = data['ner_tags'].apply(clean_ellipsis)\n",
    "\n",
    "            data = data[['tokens', 'ner_tags']]\n",
    "\n",
    "            data.to_parquet(f\"../data/parquet/{dataset}/{typ}_{lang}_{dataset}.parquet\", engine=\"fastparquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed8e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_HAREM = pd.read_parquet(f'../data/parquet/HAREM/test_pt_HAREM.parquet', engine='fastparquet')\n",
    "test_LeNER = pd.read_parquet(f'../data/parquet/LeNER-Br/test_pt_LeNER-Br.parquet', engine='fastparquet')\n",
    "test_MultL = pd.read_parquet(f'../data/parquet/MultL/test_pt_MultL.parquet', engine='fastparquet')\n",
    "\n",
    "val_HAREM = pd.read_parquet(f'../data/parquet/HAREM/val_pt_HAREM.parquet', engine='fastparquet')\n",
    "val_LeNER = pd.read_parquet(f'../data/parquet/LeNER-Br/val_pt_LeNER-Br.parquet', engine='fastparquet')\n",
    "val_MultL = pd.read_parquet(f'../data/parquet/MultL/val_pt_MultL.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23f4f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== test_HAREM ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128 entries, 0 to 127\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    128 non-null    object\n",
      " 1   ner_tags  128 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 692.2 KB\n",
      "\n",
      "=== test_LeNER ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1390 entries, 0 to 1389\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    1390 non-null   object\n",
      " 1   ner_tags  1390 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 1007.0 KB\n",
      "\n",
      "=== test_MultL ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30480 entries, 0 to 30479\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    30480 non-null  object\n",
      " 1   ner_tags  30480 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 17.4 MB\n",
      "\n",
      "=== val_HAREM ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    8 non-null      object\n",
      " 1   ner_tags  8 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 51.0 KB\n",
      "\n",
      "=== val_LeNER ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1177 entries, 0 to 1176\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    1177 non-null   object\n",
      " 1   ner_tags  1177 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 870.2 KB\n",
      "\n",
      "=== val_MultL ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30210 entries, 0 to 30209\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    30210 non-null  object\n",
      " 1   ner_tags  30210 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 17.3 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== test_HAREM ===\")\n",
    "test_HAREM.info(memory_usage=\"deep\")\n",
    "\n",
    "print(\"\\n=== test_LeNER ===\")\n",
    "test_LeNER.info(memory_usage=\"deep\")\n",
    "\n",
    "print(\"\\n=== test_MultL ===\")\n",
    "test_MultL.info(memory_usage=\"deep\")\n",
    "\n",
    "print(\"\\n=== val_HAREM ===\")\n",
    "val_HAREM.info(memory_usage=\"deep\")\n",
    "\n",
    "print(\"\\n=== val_LeNER ===\")\n",
    "val_LeNER.info(memory_usage=\"deep\")\n",
    "\n",
    "print(\"\\n=== val_MultL ===\")\n",
    "val_MultL.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147c35a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert '' with type str: tried to convert to int64\", 'Conversion failed for column ner_tags with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m all_dev \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_HAREM, val_HAREM], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mall_dev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/parquet/all/all_pt_dev_HAREM.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m all_dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m all_dev \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_LeNER, val_LeNER], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3045\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3126\u001b[0m     path,\n\u001b[0;32m   3127\u001b[0m     engine,\n\u001b[0;32m   3128\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3129\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3130\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3131\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3133\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 482\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    483\u001b[0m     df,\n\u001b[0;32m    484\u001b[0m     path_or_buf,\n\u001b[0;32m    485\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    486\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    487\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    488\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    489\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pandas\\io\\parquet.py:191\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m--> 191\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m    194\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\table.pxi:4795\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\pandas_compat.py:637\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    633\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    634\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 637\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [convert_column(c, f)\n\u001b[0;32m    638\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    640\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\pandas_compat.py:637\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    633\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    634\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 637\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    640\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\pandas_compat.py:625\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    621\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    623\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mnull_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\pandas_compat.py:619\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    616\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    621\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    623\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,)\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\array.pxi:365\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\array.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mathe\\OneDrive\\Desktop\\git\\codigos\\faculdade\\bigdata-tmp\\venv\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: (\"Could not convert '' with type str: tried to convert to int64\", 'Conversion failed for column ner_tags with type object')"
     ]
    }
   ],
   "source": [
    "all_dev = pd.concat([test_HAREM, val_HAREM], ignore_index=True)\n",
    "all_dev.to_parquet(f\"../data/parquet/all/all_pt_dev_HAREM.parquet\", engine=\"pyarrow\", index=False)\n",
    "all_dev = None\n",
    "all_dev = pd.concat([test_LeNER, val_LeNER], ignore_index=True)\n",
    "all_dev.to_parquet(f\"../data/parquet/all/all_pt_dev_LeNER.parquet\", engine=\"pyarrow\", index=False)\n",
    "all_dev = None\n",
    "all_dev = pd.concat([test_MultL, val_MultL], ignore_index=True)\n",
    "all_dev.to_parquet(f\"../data/parquet/all/all_pt_dev_MultL.parquet\", engine=\"pyarrow\", index=False)\n",
    "all_dev = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_and_val = []\n",
    "list_items = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for lang in langs[2:]:\n",
    "        for typ in data_types[:2]:\n",
    "            list_test_and_val.append(pd.read_parquet(f'../data/parquet/{dataset}/{typ}_{lang}_{dataset}.parquet', engine='fastparquet'))\n",
    "    list_items.append((dataset ,list_test_and_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa359e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list_items:\n",
    "    all_dev = pd.concat(item[1], ignore_index=True)\n",
    "\n",
    "    all_dev['ner_tags'] = all_dev['ner_tags'].replace('', pd.NA)\n",
    "    all_dev['ner_tags'] = pd.to_numeric(all_dev['ner_tags'], errors='coerce')\n",
    "    all_dev['ner_tags'] = all_dev['ner_tags'].astype('Int64')\n",
    "\n",
    "    all_dev.to_parquet(f\"../data/parquet/all/all_pt_dev_{item[0]}.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2a7661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[W, ., JAMES, Willian, James, Willian, James, ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Um, pouco, de, , Barrosão, \", .]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-, Não, faço, mistério, disso, ;, mora, com, ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLIPSEGURO, -, o, clipping, mais, SEGURO, da,...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Qual, é, o, seu, nome, ?, Ventura, Pires, da,...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  ner_tags\n",
       "0  [W, ., JAMES, Willian, James, Willian, James, ...      <NA>\n",
       "1                  [Um, pouco, de, , Barrosão, \", .]      <NA>\n",
       "2  [-, Não, faço, mistério, disso, ;, mora, com, ...      <NA>\n",
       "3  [CLIPSEGURO, -, o, clipping, mais, SEGURO, da,...      <NA>\n",
       "4  [Qual, é, o, seu, nome, ?, Ventura, Pires, da,...      <NA>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be6f7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = []\n",
    "list_items.clear()\n",
    "\n",
    "for dataset in datasets:\n",
    "    for lang in langs[2:]:\n",
    "        for typ in data_types[2:]:\n",
    "            list_train.append(pd.read_parquet(f'../data/parquet/{dataset}/{typ}_{lang}_{dataset}.parquet', engine='fastparquet'))\n",
    "    list_items.append((dataset ,list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list_items:\n",
    "    all_train = pd.concat(item[1], ignore_index=True)\n",
    "\n",
    "    all_train['ner_tags'] = all_train['ner_tags'].replace('', pd.NA)\n",
    "    all_train['ner_tags'] = pd.to_numeric(all_train['ner_tags'], errors='coerce')\n",
    "    all_train['ner_tags'] = all_train['ner_tags'].astype('Int64')\n",
    "\n",
    "    all_train.to_parquet(f\"../data/parquet/all/all_pt_train_{item[0]}.parquet\", engine=\"fastparquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
