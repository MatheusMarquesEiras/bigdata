{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0476fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94b432",
   "metadata": {},
   "source": [
    "# LeNER-Br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c5e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_typs = ['dev', 'test', 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fcb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_conll(arquivo):\n",
    "    tokens_list = []\n",
    "    tags_list = []\n",
    "    \n",
    "    tokens_sent = []\n",
    "    tags_sent = []\n",
    "    \n",
    "    with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "        for linha in f:\n",
    "            linha = linha.strip()\n",
    "            \n",
    "            if linha == \"\":\n",
    "                if tokens_sent:\n",
    "                    tokens_list.append(tokens_sent)\n",
    "                    tags_list.append(tags_sent)\n",
    "                    tokens_sent = []\n",
    "                    tags_sent = []\n",
    "                continue\n",
    "            \n",
    "            partes = linha.split()\n",
    "            if len(partes) >= 2:\n",
    "                token = partes[0]\n",
    "                tag = partes[1]\n",
    "                \n",
    "                tokens_sent.append(token)\n",
    "                tags_sent.append(tag)\n",
    "    \n",
    "    if tokens_sent:\n",
    "        tokens_list.append(tokens_sent)\n",
    "        tags_list.append(tags_sent)\n",
    "    \n",
    "    return tokens_list, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_para_numeros(tags_list):\n",
    "    tags_unicos = set()\n",
    "    for sent in tags_list:\n",
    "        tags_unicos.update(sent)\n",
    "    \n",
    "    tag2id = {tag: i for i, tag in enumerate(sorted(tags_unicos))}\n",
    "    \n",
    "    tags_ids = []\n",
    "    for sent in tags_list:\n",
    "        ids = [tag2id[tag] for tag in sent]\n",
    "        tags_ids.append(ids)\n",
    "    \n",
    "    print(\"Mapeamento de tags:\")\n",
    "    for tag, id_num in sorted(tag2id.items()):\n",
    "        print(f\"  {tag} â†’ {id_num}\")\n",
    "    \n",
    "    return tags_ids, tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7972f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_parquet(tokens_list, tags_ids, arquivo_saida):\n",
    "\n",
    "    tokens_array = pa.array(tokens_list, type=pa.list_(pa.utf8()))\n",
    "    tags_array = pa.array(tags_ids, type=pa.list_(pa.int32()))\n",
    "    \n",
    "    table = pa.Table.from_arrays(\n",
    "        [tokens_array, tags_array],\n",
    "        names=[\"tokens\", \"ner_tags\"]\n",
    "    )\n",
    "    \n",
    "    pq.write_table(table, arquivo_saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for typ in list_typs:\n",
    "    tokens_list, tags_list = ler_conll(f\"../data/raw/LeNER-Br/{typ}_pt_LeNER-Br.conll\")\n",
    "    tags_ids, tag2id = tags_para_numeros(tags_list)\n",
    "    salvar_parquet(tokens_list, tags_ids, f\"../data/parquet/LeNER-Br/{typ}_pt_LeNER-Br.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df59c0",
   "metadata": {},
   "source": [
    "# HAREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0f78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "typs_HAREM = ['mini_HAREM', 'primeiro_HAREM', 'segundo_HAREM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997f6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "for typ in typs_HAREM:\n",
    "    with open(f'../data/raw/HAREM/{typ}.json', 'r', encoding='utf-8') as arquivo:\n",
    "        data = json.load(arquivo)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop(['doc_id'], axis=1, inplace=True)\n",
    "    df.rename(columns={'doc_text': 'phase'}, inplace=True)\n",
    "    # df['entities'] = df['entities'].apply(list_to_entity_dict)\n",
    "\n",
    "    df.to_parquet(f'../data/parquet/HAREM/{typ}.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
