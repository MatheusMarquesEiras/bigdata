{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7qYGKfoMtZDR",
        "outputId": "ec419a2a-f0b7-45ca-ff00-6e3231896a5c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../data/csv/ner_dataset.csv', encoding= 'unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "sphVo4y7tqOj"
      },
      "outputs": [],
      "source": [
        "data_fillna = data.ffill(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group = (\n",
        "    data_fillna\n",
        "      .groupby('Sentence #', as_index=False)[['Word', 'POS', 'Tag']]\n",
        "      .agg(list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group['Phase'] = data_group['Word'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_dublespaces(text, char):\n",
        "    if f' {char} ' in text:\n",
        "        text = text.replace(f' {char} ', f'{char} ')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_all_pos = data['POS'].unique()\n",
        "pos_chars_punctuation = [item for item in list_all_pos.tolist() if item not in pos_chars_punctuation]\n",
        "\n",
        "for idx, row in data_group['Phase'].items():\n",
        "    for char in pos_chars_punctuation:\n",
        "        data_group.at[idx, 'Phase'] = remove_dublespaces(row, char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_group['Position'] = [[] for _ in range(len(data_group))]\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    positions = []\n",
        "    for idxW, word in enumerate(row['Word']):\n",
        "        try:\n",
        "            index = row['Phase'].index(word)\n",
        "            lenWord = len(word)\n",
        "            positions.append((index, index + lenWord, row['POS'][idxW]))\n",
        "        except ValueError:\n",
        "            # Palavra não encontrada na Phase\n",
        "            pass\n",
        "    data_group.at[idx, 'Position'] = positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, row in data_group.iterrows():\n",
        "    items = row['Position']\n",
        "    frase = list(row['Phase'])\n",
        "    for item in items:\n",
        "        if item[2] in pos_chars_punctuation:\n",
        "            index = item[0]\n",
        "            char = item[2]\n",
        "            frase[index] = char\n",
        "            row['Position'] = frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def achar_posicoes(row):\n",
        "    frase = row['Phase']\n",
        "    palavras = row['Word'] if isinstance(row['Word'], list) else [row['Word']]\n",
        "    pos_tags = row['POS'] if isinstance(row['POS'], list) else [row['POS']]\n",
        "\n",
        "    posicoes = []\n",
        "    start = 0  # garante busca progressiva para lidar com palavras repetidas\n",
        "    for w, pos in zip(palavras, pos_tags):\n",
        "        i = frase.find(w, start)\n",
        "        if i == -1:  # palavra não encontrada\n",
        "            posicoes.append((None, None, pos))\n",
        "        else:\n",
        "            posicoes.append((i, len(w) + i, pos))\n",
        "            start = i + len(w)\n",
        "    return posicoes\n",
        "\n",
        "data_group['Position'] = data_group.apply(achar_posicoes, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_finished_dataset = []\n",
        "\n",
        "for idx, row in data_group.iterrows():\n",
        "    list_finished_dataset.append((row[\"Phase\"], {\"entities\": row[\"Position\"]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "cut = int(len(list_finished_dataset) * 0.1)\n",
        "\n",
        "DEV_DATA = list_finished_dataset[:cut]\n",
        "TRAIN_DATA = list_finished_dataset[cut:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA = [item for item in TRAIN_DATA if '\\x85' not in item[0]]\n",
        "DEV_DATA = [item for item in DEV_DATA if '\\x85' not in item[0] and '\\x94' not in item[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "for _, ann in TRAIN_DATA + DEV_DATA:\n",
        "    for start, end, label in ann.get(\"entities\", []):\n",
        "        ner.add_label(label)\n",
        "\n",
        "def make_examples(nlp, data):\n",
        "    examples = []\n",
        "    for text, ann in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        spans = []\n",
        "        for start, end, label in ann[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "\n",
        "            if span is None:\n",
        "                raise ValueError(f\"Span inválido em: {text!r} -> {(start, end, label)}\")\n",
        "            \n",
        "            spans.append(span)\n",
        "            \n",
        "        doc_ents = {\"entities\": [(s.start_char, s.end_char, s.label_) for s in spans]}\n",
        "        examples.append(Example.from_dict(doc, doc_ents))\n",
        "    return examples\n",
        "\n",
        "train_examples = make_examples(nlp, TRAIN_DATA)\n",
        "dev_examples   = make_examples(nlp, DEV_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 01 - loss: 76027.9281\n",
            "Iter 02 - loss: 42598.9292\n"
          ]
        }
      ],
      "source": [
        "other_pipes = [p for p in nlp.pipe_names if p != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.initialize(get_examples=lambda: train_examples)\n",
        "    n_iter = 2\n",
        "    for itn in range(1, n_iter + 1):\n",
        "        random.shuffle(train_examples)\n",
        "        losses = {}\n",
        "        for batch in minibatch(train_examples, size=4):\n",
        "            nlp.update(batch, sgd=optimizer, drop=0.2, losses=losses)\n",
        "        if itn % 5 == 0 or itn == 1 or itn == n_iter:\n",
        "            print(f\"Iter {itn:02d} - loss: {losses.get('ner', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEV metrics: {'precision': 0.976585440262365, 'recall': 0.9762875986428272, 'f1': 0.9764364967400008}\n"
          ]
        }
      ],
      "source": [
        "def evaluate(nlp, examples):\n",
        "    pred_docs = list(nlp.pipe([ex.text for ex in examples]))\n",
        "    pred_examples = [Example(pred, ex.reference) for pred, ex in zip(pred_docs, examples)]\n",
        "\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(pred_examples)\n",
        "    return {\n",
        "        \"precision\": scores[\"ents_p\"],\n",
        "        \"recall\":    scores[\"ents_r\"],\n",
        "        \"f1\":        scores[\"ents_f\"],\n",
        "    }\n",
        "\n",
        "metrics = evaluate(nlp, dev_examples)\n",
        "print(\"DEV metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Texto: Marcos works on Apple, in São Paulo.\n",
            " - Marcos               NNP\n",
            " - works                VBZ\n",
            " - on                   IN\n",
            " - Apple                NNP\n",
            " - ,                    ,\n",
            " - in                   IN\n",
            " - São                  NNP\n",
            " - Paulo                NNP\n",
            " - .                    .\n",
            "\n",
            "Texto: the Embraer is locaded in Brasil.\n",
            " - the                  DT\n",
            " - Embraer              NNP\n",
            " - is                   VBZ\n",
            " - locaded              VBN\n",
            " - in                   IN\n",
            " - Brasil               NNP\n",
            " - .                    .\n",
            "\n",
            "Texto: Ana fly to new york.\n",
            " - Ana                  JJ\n",
            " - fly                  NN\n",
            " - to                   TO\n",
            " - new                  JJ\n",
            " - york                 NN\n",
            " - .                    .\n"
          ]
        }
      ],
      "source": [
        "tests = [\n",
        "    \"Marcos works on Apple, in São Paulo.\",\n",
        "    \"the Embraer is locaded in Brasil.\",\n",
        "    \"Ana fly to new york.\",\n",
        "]\n",
        "for t in tests:\n",
        "    doc = nlp(t)\n",
        "    print(\"\\nTexto:\", t)\n",
        "    for ent in doc.ents:\n",
        "        print(f\" - {ent.text:<20} {ent.label_}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
