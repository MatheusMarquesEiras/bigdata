{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from enum import Enum\n",
        "import re\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "prqt = pd.read_parquet('./data/parquet/MultL/test_multilingual.parquet', engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_tag(tag_id):\n",
        "    if tag_id in range(1, 3):\n",
        "        return \"PER\"\n",
        "    elif tag_id in range(3, 5):\n",
        "        return \"ORG\"\n",
        "    elif tag_id in range(5, 7):\n",
        "        return \"LOC\"\n",
        "    elif tag_id in range(7, 9):\n",
        "        return \"MIST\"\n",
        "    else:\n",
        "        return \"O\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_tokens_to_spacy_format(tokens, tags):\n",
        "    text = \"\"\n",
        "    entities = []\n",
        "    token_start = 0\n",
        "\n",
        "    for i, (token, tag_id) in enumerate(zip(tokens, tags)):\n",
        "        tag = map_tag(tag_id)\n",
        "\n",
        "        # Calcula os offsets\n",
        "        start = len(text)\n",
        "        text += token\n",
        "        end = len(text)\n",
        "\n",
        "        if i < len(tokens) - 1:\n",
        "            text += \" \"\n",
        "\n",
        "        if tag != \"O\":\n",
        "            # Se for entidade, adiciona aos spans\n",
        "            entities.append((start, end, tag))\n",
        "\n",
        "    return (text.strip(), {\"entities\": entities})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"On this occasion he failed to gain the support of the South Wales Miners ' Federation and had to stand down .\", {'entities': [(54, 59, 'ORG'), (60, 65, 'ORG'), (66, 72, 'ORG'), (73, 74, 'ORG'), (75, 85, 'ORG')]})\n",
            "(\"On both these occasions he was backed by the South Wales Miners ' Federation , but he was not successful .\", {'entities': [(45, 50, 'ORG'), (51, 56, 'ORG'), (57, 63, 'ORG'), (64, 65, 'ORG'), (66, 76, 'ORG')]})\n",
            "('He also appeared as himself in the 1996 film \" Eddie \" .', {'entities': [(47, 52, 'MIST')]})\n",
            "('The Colorado Rockies were created as an expansion franchise in 1993 and Coors Field opened in 1995 .', {'entities': [(4, 12, 'ORG'), (13, 20, 'ORG'), (72, 77, 'LOC'), (78, 83, 'LOC')]})\n",
            "('He kept busy recording demo tapes at his home and working various jobs , including a position as a contracted security guard at the La Valencia Hotel in La Jolla .', {'entities': [(132, 134, 'LOC'), (135, 143, 'LOC'), (144, 149, 'LOC'), (153, 155, 'LOC'), (156, 161, 'LOC')]})\n"
          ]
        }
      ],
      "source": [
        "data_spacy = [\n",
        "    convert_tokens_to_spacy_format(row[\"tokens\"], row[\"ner_tags\"])\n",
        "    for _, row in prqt.iterrows()\n",
        "]\n",
        "\n",
        "for item in data_spacy[:5]:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA = data_spacy[:int(len(data_spacy) * 0.90)]\n",
        "DEV_DATA = data_spacy[:int(len(data_spacy) * 0.10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_docbin(data, file_path):\n",
        "    nlp = spacy.blank(\"pt\")\n",
        "    db = DocBin()\n",
        "\n",
        "    print(f\"Gerando dados para {file_path}...\")\n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        for start, end, label in annot[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label)\n",
        "            if span is not None:\n",
        "                ents.append(span)\n",
        "        doc.ents = ents\n",
        "        db.add(doc)\n",
        "\n",
        "    db.to_disk(file_path)\n",
        "    print(f\"Dados salvos em '{file_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gerando dados para train_data.spacy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28237/28237 [00:03<00:00, 9265.63it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados salvos em 'train_data.spacy'\n",
            "Gerando dados para dev_data.spacy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3137/3137 [00:00<00:00, 11197.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados salvos em 'dev_data.spacy'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "create_docbin(TRAIN_DATA, \"train_data.spacy\")\n",
        "create_docbin(DEV_DATA, \"dev_data.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nossa LOC\n",
            "Senhora LOC\n",
            "Aparecida LOC\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"./model/model-best\")\n",
        "\n",
        "doc = nlp(\"Nossa Senhora Aparecida\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "if \"textcat\" in nlp.pipe_names:\n",
        "    print(doc.cats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy train ./config.cfg --output ./model --paths.train ./traning/train_data.spacy --paths.dev ./traning/dev_data.spacy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
